{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv(dotenv_path=\"../keys/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openAI_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# List of PDF file paths\n",
    "pdf_dir = \"../data/pdfs\"\n",
    "pdf_files = [os.path.join(pdf_dir, file) for file in os.listdir(pdf_dir) if file.endswith(\".pdf\")]\n",
    "\n",
    "# URLs to load\n",
    "urls = [\n",
    "    \"https://www.math-datascience.nat.fau.de/im-studium/masterstudiengaenge/master-data-science/\",\n",
    "    \"https://www.fau.eu/studiengang/data-science-bsc/\",\n",
    "    \"https://www.fau.eu/studiengang/data-science-msc/\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to hold all document splits\n",
    "all_doc_splits = []\n",
    "\n",
    "# Define the text splitter with a chunk size of 1000 characters and overlap of 200\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200)\n",
    "\n",
    "### Process PDF files ###\n",
    "for pdf_file in pdf_files:\n",
    "    # Load the PDF\n",
    "    pdf_loader = PyPDFLoader(pdf_file)\n",
    "    documents = pdf_loader.load()\n",
    "    \n",
    "    # Split the documents into chunks\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Add the splits to the overall list\n",
    "    all_doc_splits.extend(doc_splits)\n",
    "\n",
    "### Process Web URLs ###\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split the web documents into chunks\n",
    "web_doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add the web document splits to the overall list\n",
    "all_doc_splits.extend(web_doc_splits)\n",
    "\n",
    "# Now, all_doc_splits contains the splits from both the PDF files and the web documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "print(len(all_doc_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=all_doc_splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines and give only questions not anything else, not even statements like Here are five search queries:. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatGroq(model_name=\"llama-3.1-70b-versatile\") \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    # 'Will the language of instruction of my Bachelor’s degree suffice to demonstrate my proficiency in English for admission in Msc Data Science at FAU Erlangen?',\n",
    "    # 'Is an APS certificate mandatory for Indian students during the application process?',\n",
    "    # 'How long does it typically take to receive a decision regarding my application to the MSc Data Science program?',\n",
    "    # 'Am I eligible to apply for the study course program with a degree from a technical university or a university of applied sciences?',\n",
    "    # 'Can I apply for the study course program with a degree obtained through a dual study program?',\n",
    "    # 'What could be the reasons for receiving a rejection for my application?',\n",
    "    # 'Is it possible to defer my admission to a future semester?',\n",
    "    # 'Where can I find accommodation while studying at FAU?',\n",
    "    # 'Can I switch my major subject after being admitted?',\n",
    "    # 'Is it allowed to combine modules from different application subjects?',\n",
    "    # 'How can I transfer ECTS credits from one module group to another?',\n",
    "    # 'Can modules from my major subject count towards my minor subject requirements?',\n",
    "    # 'Can my werkstudent experience be used to earn ECTS as a technical qualification?',\n",
    "    # 'Can I participate in an examination without prior registration?',\n",
    "    # 'How do I register for a module’s examination in Campo within the correct specialisation area?',\n",
    "    # 'If I fail an examination on the first attempt, is it mandatory to attend the next attempt?',\n",
    "    # 'Is it possible to withdraw from an examination after registering?',\n",
    "    # 'What should I do if I cannot attend an examination due to illness?',\n",
    "    # 'What are the consequences of failing an examination three times?',\n",
    "    # 'Can I retake a completed module to improve my grades?',\n",
    "    # 'Is it possible to improve my grades by completing additional modules?',\n",
    "    # 'How many ECTS credits are required before I can start writing my Master’s thesis?',\n",
    "    # 'How can I find a topic for my Master’s thesis?',\n",
    "    # 'Who is eligible to supervise my Master’s thesis?',\n",
    "    # 'Can my Master’s thesis supervisor be from a different department?',\n",
    "    # 'Is it possible to complete my Master’s thesis while working in a company?',\n",
    "    # 'Is it necessary to have a university supervisor for a company-based thesis?',\n",
    "    # 'What is the process for registering my Master’s thesis?',\n",
    "    # 'What steps should I take after completing my Master’s thesis?',\n",
    "\n",
    "    # 'What are the admission requirements of M.Sc. Data Science at FAU Erlangen?',\n",
    "    # 'How can I apply for M.Sc. Data Science at FAU Erlangen?',\n",
    "    # 'Is accommodation available at FAU Erlangen?',\n",
    "    # 'How can I finance my studies at FAU Erlangen?',\n",
    "    # 'What are the next steps if I get an admission at FAU Erlangen?',\n",
    "    # 'What is the duration of M.Sc. Data Science degree program at FAU Erlangen?',\n",
    "    # 'What is the teaching language of M.Sc. Data Science at FAU Erlangen?',\n",
    "    # 'What is the structure of M.Sc. Data Science at FAU Erlangen?',\n",
    "    # 'Is German required for M.Sc. Data Science at FAU Erlangen?',\n",
    "    # 'What is the duration of B.Sc. Data Science degree program at FAU Erlangen?',\n",
    "    # 'What is the teaching language of B.Sc. Data Science at FAU Erlangen?',\n",
    "    'What is the structure of B.Sc. Data Science FAU Erlangen?',\n",
    "    # 'What are the admission requirements of B.Sc. Data Science FAU Erlangen?'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ekaansh_Khosla\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for question in questions:\n",
    "    retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "    docs = retrieval_chain.invoke({\"question\":question})\n",
    "\n",
    "    from operator import itemgetter\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "    # RAG\n",
    "    template = \"\"\"Answer the following question in detail and based on this context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    final_rag_chain = (\n",
    "        {\"context\": retrieval_chain, \n",
    "        \"question\": itemgetter(\"question\")} \n",
    "        | prompt\n",
    "        | ChatGroq(model_name=\"llama-3.1-70b-versatile\")\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    ans = final_rag_chain.invoke({\"question\":question})\n",
    "    answers.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The B.Sc. Data Science program at FAU Erlangen-Nürnberg is structured into several consecutive levels. \\n\\n1. The first level focuses on foundational courses in mathematics and computer science, which provide a solid understanding of the basics of these subjects. \\n2. The second level builds upon the foundational knowledge by offering advanced courses in mathematics and computer science. \\n3. The third level allows students to specialize in a specific application subject, such as physics, business informatics, biology, or medical technology. \\n4. The fourth level consists of a bachelor's thesis, which requires students to apply their knowledge and skills to a real-world problem.\\n\\nAdditionally, the program offers a semester abroad option, which allows students to spend one semester at another European university, such as through the ERASMUS program. The standard period of study for the bachelor's degree program is six semesters.\\n\\nThe program is designed to provide students with a solid foundational and advanced education in both mathematics and computer science, with a strong focus on the requirements of future Data Scientists.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
